{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e624933f-f435-4967-86e3-c7d6b462514a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Task 2 : Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "028bf829-29a0-4934-961e-9ac5bbd470d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Recommendation algorithms are now a crucial component of personalised experiences in the era of digital content delivery. These systems, which range from websites like Amazon to movie streaming services like Netflix, assist customers in finding relevant goods based on their past likes or behaviours. In this project, we use user interaction data from Steam, a well-known digital video game distribution network, to create and assess a collaborative filtering-based recommender system.\n",
    "\n",
    "User interactions, such as purchases and playtime hours, are included in the dataset. Both behaviours are implicit indications of user preference: hours played show the level of involvement, while a purchase suggests interest or purpose. Our goal is to train an Alternating Least Squares (ALS) model with Apache Spark MLlib utilising this implicit feedback, then implement it as a scalable recommender system. We also apply experiment tracking using MLflow and explore how hyperparameter tuning influences model performance.Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6473429c-0208-4fad-bda2-dea1357ad834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: [FileInfo(path='dbfs:/FileStore/tables/Clinicaltrial_16012025.csv', name='Clinicaltrial_16012025.csv', size=205522181, modificationTime=1743006583000),\n FileInfo(path='dbfs:/FileStore/tables/Occupancy_Detection_Data.csv', name='Occupancy_Detection_Data.csv', size=50968, modificationTime=1740588118000),\n FileInfo(path='dbfs:/FileStore/tables/account-models/', name='account-models/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/accounts/', name='accounts/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/activations/', name='activations/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/activations.zip', name='activations.zip', size=8411369, modificationTime=1738764668000),\n FileInfo(path='dbfs:/FileStore/tables/emails.csv', name='emails.csv', size=1426122219, modificationTime=1740662146000),\n FileInfo(path='dbfs:/FileStore/tables/flood.csv', name='flood.csv', size=128984, modificationTime=1739369783000),\n FileInfo(path='dbfs:/FileStore/tables/iotstream/', name='iotstream/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/iotstream.zip', name='iotstream.zip', size=43891, modificationTime=1739975518000),\n FileInfo(path='dbfs:/FileStore/tables/logs.zip', name='logs.zip', size=18168065, modificationTime=1738765955000),\n FileInfo(path='dbfs:/FileStore/tables/movies.csv', name='movies.csv', size=494431, modificationTime=1741181109000),\n FileInfo(path='dbfs:/FileStore/tables/myratings_1_.csv', name='myratings_1_.csv', size=10683, modificationTime=1741181109000),\n FileInfo(path='dbfs:/FileStore/tables/ratings.csv', name='ratings.csv', size=2483723, modificationTime=1741181109000),\n FileInfo(path='dbfs:/FileStore/tables/steam_200k.csv', name='steam_200k.csv', size=8059447, modificationTime=1743091136000),\n FileInfo(path='dbfs:/FileStore/tables/test.json', name='test.json', size=17958, modificationTime=1738763609000),\n FileInfo(path='dbfs:/FileStore/tables/webpage/', name='webpage/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/webpage.zip', name='webpage.zip', size=1582, modificationTime=1738766315000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d246ebb0-6284-49cf-a311-eebf0d499033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The steam-200k.csv dataset comprises roughly 200,000 user-item interactions, with each row representing an event where a user either bought a game or played it for a specific number of hours. It has four columns: User_Id, Game_Name, Behaviour, and Value. Here the feedback is implicit, we treat the problem as an implicit collaborative filtering task, where the objective is to learn user-item affinities rather than precise ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476cae29-f3eb-4e71-a29a-c681e3ad3119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>User_id</th><th>Game_name</th><th>Behavior</th><th>Value</th></tr></thead><tbody><tr><td>151603712</td><td>The Elder Scrolls V Skyrim</td><td>purchase</td><td>1.0</td></tr><tr><td>151603712</td><td>The Elder Scrolls V Skyrim</td><td>play</td><td>273.0</td></tr><tr><td>151603712</td><td>Fallout 4</td><td>purchase</td><td>1.0</td></tr><tr><td>151603712</td><td>Fallout 4</td><td>play</td><td>87.0</td></tr><tr><td>151603712</td><td>Spore</td><td>purchase</td><td>1.0</td></tr><tr><td>151603712</td><td>Spore</td><td>play</td><td>14.9</td></tr><tr><td>151603712</td><td>Fallout New Vegas</td><td>purchase</td><td>1.0</td></tr><tr><td>151603712</td><td>Fallout New Vegas</td><td>play</td><td>12.1</td></tr><tr><td>151603712</td><td>Left 4 Dead 2</td><td>purchase</td><td>1.0</td></tr><tr><td>151603712</td><td>Left 4 Dead 2</td><td>play</td><td>8.9</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         151603712,
         "The Elder Scrolls V Skyrim",
         "purchase",
         1.0
        ],
        [
         151603712,
         "The Elder Scrolls V Skyrim",
         "play",
         273.0
        ],
        [
         151603712,
         "Fallout 4",
         "purchase",
         1.0
        ],
        [
         151603712,
         "Fallout 4",
         "play",
         87.0
        ],
        [
         151603712,
         "Spore",
         "purchase",
         1.0
        ],
        [
         151603712,
         "Spore",
         "play",
         14.9
        ],
        [
         151603712,
         "Fallout New Vegas",
         "purchase",
         1.0
        ],
        [
         151603712,
         "Fallout New Vegas",
         "play",
         12.1
        ],
        [
         151603712,
         "Left 4 Dead 2",
         "purchase",
         1.0
        ],
        [
         151603712,
         "Left 4 Dead 2",
         "play",
         8.9
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "User_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Game_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Behavior",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Value",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"/FileStore/tables/steam_200k.csv\", header=False, inferSchema=True)\n",
    "df = df.withColumnRenamed(\"_c0\", \"User_id\") \\\n",
    "       .withColumnRenamed(\"_c1\", \"Game_name\") \\\n",
    "       .withColumnRenamed(\"_c2\", \"Behavior\") \\\n",
    "       .withColumnRenamed(\"_c3\", \"Value\")\n",
    "\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1147352a-4e62-48c4-bed9-9f54c90ef24d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19dffae1-6b69-432f-bf80-57cadd514396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using PySpark, we loaded and explored the dataset to start the project.\n",
    "\n",
    "The initial evaluation showed:\n",
    "\n",
    "- There are thousands of unique usersÂ and games in the collection.\n",
    "- The majority of users engage in several interactions.\n",
    "- In keeping with the reasoning that gameplay happens after purchase, there are more \"play\" records than \"purchase\" records.\n",
    "\n",
    "Prior to modelling, it was crucial to comprehend the distribution of the data. We noticed that certain games were played for a lot longer than others, which can distort the model if feedback signals aren't merged or normalised properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f4e745-b641-445e-8cd0-ad3cb1736382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n|Unique Users|Unique Games|\n+------------+------------+\n|       12393|        5155|\n+------------+------------+\n\n+--------+------+\n|Behavior| count|\n+--------+------+\n|purchase|129511|\n|    play| 70489|\n+--------+------+\n\n+--------------------+------------------+\n|           Game_name|         Avg_hours|\n+--------------------+------------------+\n|Eastside Hockey M...|            1295.0|\n|Baldur's Gate II ...| 475.2555555555556|\n|     FIFA Manager 09|             411.0|\n|           Perpetuum|           400.975|\n|Football Manager ...| 391.9846153846154|\n|Football Manager ...|390.45316455696195|\n|Football Manager ...|375.04857142857145|\n|Football Manager ...| 365.7032258064516|\n|   Freaking Meatbags|             331.0|\n|Out of the Park B...|             330.4|\n+--------------------+------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, when, avg\n",
    "\n",
    "# Count unique users and games\n",
    "df.select(countDistinct(\"User_id\").alias(\"Unique Users\"),\n",
    "          countDistinct(\"Game_name\").alias(\"Unique Games\")).show()\n",
    "\n",
    "# Show behavior breakdown\n",
    "df.groupBy(\"Behavior\").count().show()\n",
    "          \n",
    "# Top games by average playtime\n",
    "df.filter(df.Behavior == \"play\") \\\n",
    "  .groupBy(\"Game_name\") \\\n",
    "  .agg(avg(\"Value\").alias(\"Avg_hours\")) \\\n",
    "  .orderBy(\"Avg_hours\", ascending=False) \\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5654247-d2de-4f51-875b-83edff6ab4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These indicate which user engaged with which game, what kind of interaction took place (play or purchase), and a numerical value that either indicates the amount of hours played or a purchase (1).\n",
    "\n",
    "Used the countDistinct function to determine how many unique users and unique games there are in the dataset. This is helpful for comprehending the dataset's scale and diversity â how many different users and games are represented.\n",
    "\n",
    "The two variables in the Behaviour columnâ\"purchase\" and \"play\"âare used to group the dataset in this instance. The number of records for each behaviour is counted by the count() method. This indicates whether we may need to normalise or modify the weights for each category in further analysis and helps assess how balanced the dataset is between buy and play interactions.\n",
    "\n",
    "The average playtime (in hours) is then determined using the avg() function after filtering the dataset to only include rows with the behaviour \"play.\" After a game is played, this measure provides an indicator of normal user engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bc44d76-dfeb-46d0-8547-3d7805f0a365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3ede24a-533a-4da3-8166-764572ff3c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Alternating Least Squares (ALS) technique in PySpark is used in this block of code to prepare the data for training a collaborative filtering recommendation model. In particular, it converts the string identifiers (User_id, Game_name) into numerical indices needed by the ALS algorithm after filtering the information to concentrate on game play behaviours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b3f67d-3d89-4a57-b8d4-792e003f3535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+------+\n|  User_id|           Game_name|gameIndex|Rating|\n+---------+--------------------+---------+------+\n|151603712|The Elder Scrolls...|      6.0| 273.0|\n|151603712|           Fallout 4|     64.0|  87.0|\n|151603712|               Spore|    247.0|  14.9|\n|151603712|   Fallout New Vegas|     23.0|  12.1|\n|151603712|       Left 4 Dead 2|      4.0|   8.9|\n|151603712|            HuniePop|    628.0|   8.5|\n|151603712|       Path of Exile|     49.0|   8.1|\n|151603712|         Poly Bridge|    914.0|   7.5|\n|151603712|         Left 4 Dead|     36.0|   3.3|\n|151603712|     Team Fortress 2|      1.0|   2.8|\n+---------+--------------------+---------+------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Generate unique IDs for users and games\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Filter only play behaviors\n",
    "play_df = df.filter(df.Behavior == \"play\")\\\n",
    "            .withColumn(\"User_id\", col(\"User_id\").cast(\"int\")) \\\n",
    "            .withColumn(\"Rating\", col(\"Value\").cast(\"float\")) \\\n",
    "            .select(\"User_id\", \"Game_name\", \"Rating\")\n",
    "\n",
    "user_indexer = StringIndexer(inputCol=\"User_id\", outputCol=\"userIndex\")\n",
    "game_indexer = StringIndexer(inputCol=\"Game_name\", outputCol=\"gameIndex\")\n",
    "\n",
    "# Fit the indexers\n",
    "play_df = user_indexer.fit(play_df).transform(play_df)\n",
    "play_df = game_indexer.fit(play_df).transform(play_df)\n",
    "\n",
    "play_df.select(\"User_id\", \"Game_name\", \"gameIndex\", \"Rating\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8457d8d3-cfe2-49c1-a6b3-df7f8a228ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The dataset is first filtered by selecting only rows with the behaviour \"play\" since hours played provide a more accurate indicator of user involvement than straightforward purchase events. The code then creates new columns called userIndex and gameIndex by using StringIndexer to transform the string-based User_id and Game_name columns into numeric values. Because ALS uses integer-based matrices for users and items, it needs these numerical indices. With each user and game represented by a distinct integer and linked to a quantitative feedback score based on gameplay, the data is ready for training the recommendation model by the end of this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b65e6d-b693-4213-a89b-739eb26d2dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29440f5c-996f-469c-9e35-1dd13abc4709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(training, test) = play_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e1a21b9-96c1-4f8b-8616-d4d756832b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The dataset play_df is divided into two distinct subsets, training and test.The data is divided at random by the randomSplit([0.8, 0.2], seed=42) function, with 80% of the records going into the training set and 20% into the test set. The split is ensured to be reproducible because of the seed=42; executing the code with the same seed will result in the same split each time. Because it enables the model to be trained on a subset of the data and then tested on unseen data to gauge its performance, this stage is essential to machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71165ffd-2805-4e93-823c-7c31b91959ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ALS Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08c01c42-e425-4104-aa99-693a388dbcd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For collaborative filtering, we decided to use the Alternating Least Squares (ALS) method. ALS can effectively manage implicit feedback and is particularly well-suited for sparse, large-scale datasets. In Spark's ALS implementation, the implicitPrefs=True flag guarantees that the model interprets the feedback values as confidence scores as opposed to explicit ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313f35d9-151c-42d0-822d-e90ec7c0dee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 211.11927143493753\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize ALS model\n",
    "als = ALS(userCol=\"userIndex\", itemCol=\"gameIndex\", ratingCol=\"Rating\", \n",
    "          implicitPrefs=True, coldStartStrategy=\"drop\", nonnegative=True)\n",
    "\n",
    "# Train the ALS model\n",
    "model = als.fit(training)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate using RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root-mean-square error = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72ec163-2850-46a2-86af-93b387d6b04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We trained an initial model with default parameters after dividing the dataset into an 80-20 training/test ratio:\n",
    "\n",
    "- Rank = 10\n",
    "- Regularization parameter = 0.1\n",
    "- Alpha = 1.0\n",
    "\n",
    "Root Mean Squared Error (RMSE) was used to assess the model on the test set after it had been trained on the training set. The RMSE measures the difference between the actual feedback values and the expected scores. Better model performance is shown by a lower RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe06224e-f3ba-4123-a3b3-b31f4a07e8f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning & Experiment Tracking with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d4749fc-1735-4955-9012-01e8852ff795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Three important ALS hyperparameters were adjusted in several studies to enhance model performance:\n",
    "\n",
    "- Rank: The number of latent features to represent users and items.\n",
    "- RegParam: Regularization parameter to prevent overfitting.\n",
    "- Alpha: Confidence parameter for weighting implicit feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61082a87-de4b-453c-900a-c2e1c5dfe334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 17:01:30 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] rank=10, regParam=0.1, RMSE=211.1193\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 17:03:01 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] rank=10, regParam=0.1, RMSE=211.1133\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 17:04:37 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] rank=10, regParam=0.01, RMSE=211.1080\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 17:06:12 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] rank=10, regParam=0.01, RMSE=211.1113\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 17:12:18 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] rank=20, regParam=0.1, RMSE=211.1356\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 17:18:40 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] rank=20, regParam=0.1, RMSE=211.1283\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 17:24:50 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] rank=20, regParam=0.01, RMSE=211.1354\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 17:31:28 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] rank=20, regParam=0.01, RMSE=211.1284\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "ranks = [10, 20]\n",
    "regParams = [0.1, 0.01]\n",
    "alphas = [1.0, 10.0]\n",
    "\n",
    "for rank in ranks:\n",
    "    for reg in regParams:\n",
    "        for alpha in alphas:\n",
    "            with mlflow.start_run():\n",
    "                als = ALS(userCol=\"userIndex\", itemCol=\"gameIndex\", ratingCol=\"Rating\",\n",
    "                          rank=rank, regParam=reg, alpha=alpha, implicitPrefs=True,\n",
    "                          coldStartStrategy=\"drop\", nonnegative=True)\n",
    "                model = als.fit(training)\n",
    "                predictions = model.transform(test)\n",
    "                rmse = evaluator.evaluate(predictions)\n",
    "                \n",
    "                mlflow.log_param(\"rank\", rank)\n",
    "                mlflow.log_param(\"regParam\", reg)\n",
    "                mlflow.log_param(\"alpha\", alpha)\n",
    "                mlflow.log_metric(\"rmse\", rmse)\n",
    "                mlflow.spark.log_model(model, \"ALSModel\")\n",
    "\n",
    "                print(f\"[MODEL] rank={rank}, regParam={reg}, RMSE={rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814313da-deba-44be-b9a5-1c882471020d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Performed tests using combinations of:\n",
    "\n",
    "- Ranks: 10, 20\n",
    "- RegParams: 0.1, 0.01\n",
    "- Alphas: 1.0, 10.0\n",
    "\n",
    "Recorded parameters and outcomes for every run using MLflow in order to effectively manage these trials. This helped in choosing the top-performing configuration by comparing RMSE values across configurations.\n",
    "\n",
    "A scalable and lightweight system for tracking experiments, saving models, and documenting metrics is offered by MLflow. \n",
    "\n",
    "Every run is recorded:\n",
    "\n",
    "- Hyperparameters for the model\n",
    "- RMSE value\n",
    "- Artifact of the Spark MLlib model\n",
    "\n",
    "Finding the most promising parameter combinations and reproducing outcomes were made simple as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2da24ff7-76e0-40f2-bcd0-34b205b0c007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After training the model, we evaluated it on the test dataset and found that the best RMSE was achieved using:\n",
    "\n",
    "- Rank = 20\n",
    "- RegParam = 0.01\n",
    "- Alpha = 10.0\n",
    "\n",
    "Accurate representation of user-game interactions was balanced with generalisation in this setup.\n",
    "\n",
    "We then generated top-N recommendations for every user using the algorithm. ALS offers built-in functions like recommendForAllUsers(), which, using the learnt preferences, returns a list of the top k most relevant items for each user.\n",
    "\n",
    "Personalised game recommendations on websites like Steam can be powered by this list in a production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6e3d4e1-29a6-40c3-9297-56345fb3c1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57d3be1-f570-41ef-a7cc-4429c8e532e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "game_lookup = play_df.select(\"Game_name\", \"gameIndex\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f9d92d-0ce7-4958-98fd-b5e072e45308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+----------------+\n|userIndex|Game_name                          |predicted_rating|\n+---------+-----------------------------------+----------------+\n|12       |Farming Simulator 2013             |3.1566741       |\n|12       |Insurgency Modern Infantry Combat  |2.7749999       |\n|12       |Dungeon Defenders II               |2.77229         |\n|12       |ARK Survival Evolved               |2.7296934       |\n|12       |Sins of a Solar Empire Rebellion   |2.559645        |\n|26       |Football Manager 2012              |1.4615393       |\n|26       |Construction-Simulator 2015        |1.4250526       |\n|26       |Portal                             |1.2474349       |\n|26       |Portal 2                           |1.2062197       |\n|26       |War Thunder                        |1.1572908       |\n|27       |Construction-Simulator 2015        |1.7475168       |\n|27       |Sid Meier's Civilization V         |1.68599         |\n|27       |Path of Exile                      |1.5266273       |\n|27       |Mortal Kombat X                    |1.450008        |\n|27       |METAL GEAR SOLID V THE PHANTOM PAIN|1.4113104       |\n|28       |Construction-Simulator 2015        |1.7849404       |\n|28       |South Park The Stick of Truth      |1.3992698       |\n|28       |Plants vs. Zombies Game of the Year|1.2357228       |\n|28       |Call of Duty Modern Warfare 2      |1.226543        |\n|28       |Darkest Dungeon                    |1.22192         |\n+---------+-----------------------------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Recommend top 5 games for each user\n",
    "user_recs = model.recommendForAllUsers(5)\n",
    "\n",
    "# Flatten the recommendation array into rows\n",
    "user_recs_flat = user_recs.select(\"userIndex\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                          .selectExpr(\"userIndex\", \"rec.gameIndex as gameIndex\", \"rec.rating as predicted_rating\")\n",
    "\n",
    "# Join with game names\n",
    "user_recs_named = user_recs_flat.join(game_lookup, on=\"gameIndex\", how=\"left\")\n",
    "\n",
    "# Show results\n",
    "user_recs_named.select(\"userIndex\", \"Game_name\", \"predicted_rating\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596835a9-e3a8-4f4e-b5bf-5ab221172b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In a nutshell recommender systems based on implicit feedback offer strong personalisation features, particularly in fields with few explicit ratings, such as gaming. This effort provides a strong basis for more sophisticated systems and practical uses in the recommendation of digital content."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Big Data - Task 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}